{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# BC205: Algorithms for Bioinformatics.\n",
    "## VI. Rapid Searches\n",
    "\n",
    "### Christoforos Nikolaou\n",
    "\n",
    "### Introduction\n",
    "\n",
    "#### Sequence similarity. Up to now:\n",
    "\n",
    "Dealing with the problem of sequence similarity and comparison, we have up to now seen how:\n",
    "* We can quantify the similarity between two sequences using scoring schemes that are either arbitrarily defined, or based on **substitution matrices** obtained from molecular evolution approaches.\n",
    "* We can define an objective methodology that maximizes sequence similarity through the concept of **alignment**.\n",
    "* We can distinguish between **global** (for the entire sequence length) and **local** (for subparts of the sequences) alignment.\n",
    "\n",
    "#### More complex problems\n",
    "We are now moving on into a different aspect of the problem of sequence similarity that stems from the scale of sequence size and volume.  \n",
    "\n",
    "Consider the following problems:\n",
    "\n",
    "1. We need to check the similarity not between two sequences but between one sequence and a much longer one (such as, for instance, a whole genome).\n",
    "2. We would like to search for similarity for a given sequence in a database that contains hundreds of thousands, or millions of sequences.  \n",
    "  \n",
    "In both of the above problems we cannot proceed with sequence alignment in the way we have seen up to now. In fact we need to consider rather different strategies for rapid sequence matches. In this and the next week, we will discuss:\n",
    "\n",
    "a. How we may approach the problem of similarity with identical matches. These sound more restrictive than the alignment concept but we will see how we can apply alignment strategies on selected identical matching \"seeds\" that effectively help us speed up the whole process.\n",
    "\n",
    "b. How we can use data transformation techniques to enable the speeding up and paralelization of sequence searches.\n",
    "\n",
    "### Pattern Searches\n",
    "\n",
    "We will start from the simplest, yet fundamental approach in the string matching problem. Consider a long sequence we will call _sequence_, and a smaller one we will _pattern_. Our goal is to write a program that will identify matches of _pattern_ within _sequence_. Even though we are tempted to use pre-defined functions, it is a good exercise to try and think how we would do this from scratch.  \n",
    "\n",
    "### Naive Pattern Search\n",
    "\n",
    "Consider the simplest way possible. \n",
    "\n",
    "We start from the beginning of _sequence_ and scan substrings of length equal to pattern for one-to-one matches. We do this exhaustively for all substrings. Thus, the steps are:\n",
    "\n",
    "1. Start from _sequence[0]_ and loop one residue at a time (i=0)    \n",
    "2. Take a substring from _sequence_ equal to _pattern_  \n",
    "3. Start from _pattern[0]_ and compare to _sequence[i]_ (j=0)\n",
    "4. If there is a mismatch, exit and go to 2, take next _i_\n",
    "5. If there are no mismatches and _j_ reaches the size of length of _pattern_, report a full match, go to 2 and take next _i_  \n",
    "  \n",
    "Below is a Python script to do this in the simplest way possible. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pattern found at index  5\nPattern found at index  25\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('Number of steps taken=', 26)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# Naive Pattern Searching algorithm\n",
    "def naivePatternSearch(pattern, sequence):\n",
    "    p = len(pattern)\n",
    "    s = len(sequence)\n",
    "    no = 0\n",
    " \n",
    "    # We slide pattern one residue/character at a time\n",
    "    for i in range(s - p + 1):\n",
    "        no += 1\n",
    "        j = 0\n",
    "        \n",
    "        # for each pairing of pattern to sequence we check characters starting from the beginning\n",
    "        while(j < p):\n",
    "            if (sequence[i + j] != pattern[j]): \n",
    "                break\n",
    "            j += 1\n",
    " \n",
    "        if (j == p):\n",
    "            print(\"Pattern found at index \", i)\n",
    "    return(\"Number of steps taken=\", no)\n",
    " \n",
    "naivePatternSearch('abba', 'IhateabbaandIdontlikealibabba')"
   ]
  },
  {
   "source": [
    "### Naive Pattern Search. Limitations\n",
    "\n",
    "Can you spot problems with the approach above that make it slow and inefficient? In which algorithm category do you think it falls?\n",
    "\n",
    "### Optimized naive search\n",
    "\n",
    "There are actually two points that we should consider. One is that in a case of a long pattern that matches the fist _p-1_ positions but not in the final one, we may be doing a lot of comparisons we can do away with.\n",
    "The other is that we may be able to speed up the process provided that the _pattern_ has some particular properties, by sliding not 1 character at a time, but more. The condition is for the _pattern_ to be variable because when all characters of the _pattern_ are different, we can slide the pattern by more than 1. This is because when a mismatch occurs after _j_ matches, we know that the first character of pattern will not match the j matched characters because all characters of pattern are different. So we can always slide the pattern **not by 1 but by j** without missing any valid shifts. \n",
    "\n",
    "Following is the modified code that is optimized for the special patterns. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pattern found at index 5\nPattern found at index 25\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('Number of steps taken=', 23)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Optimized Naive Search \n",
    "def optNaivePatternSearch(pattern, sequence):\n",
    "    p = len(pattern)\n",
    "    s = len(sequence)\n",
    "    i = 0\n",
    "    no = 0\n",
    "  \n",
    "    while i <= s-p:\n",
    "        no+=1\n",
    "        # For current index i, check for pattern match\n",
    "        for j in range(p):\n",
    "            if sequence[i+j] != pattern[j]:\n",
    "                break\n",
    "            j += 1\n",
    "  \n",
    "        if j==p:    # if pat[0...M-1] = txt[i,i+1,...i+M-1]\n",
    "            print(\"Pattern found at index \" + str(i))\n",
    "            i = i + p\n",
    "        elif j==0:\n",
    "            i = i + 1\n",
    "        else:\n",
    "            i = i+ j    # slide the pattern by j\n",
    "    return(\"Number of steps taken=\", no)\n",
    "\n",
    "optNaivePatternSearch('abba', 'IhateabbaandIdontlikealibabba')\n"
   ]
  },
  {
   "source": [
    "Notice how the steps taken for the optimized approach are 3 fewer than the naive one. This is even stronger when the pattern is more variable than above. Compare:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pattern found at index  9\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('Number of steps taken=', 27)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "naivePatternSearch('ledzeppelin', 'IhateabbaledzeppeliniswhatIreallylove')\n",
    "\n"
   ]
  },
  {
   "source": [
    "with:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pattern found at index 9\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('Number of steps taken=', 17)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "optNaivePatternSearch('ledzeppelin', 'IhateabbaledzeppeliniswhatIreallylove')"
   ]
  },
  {
   "source": [
    "You see that a pattern of large size makes the situation much faster since we don't need to slide it exhaustively. \n",
    "A number of exact pattern matching approaches are based on efficient pattern sliding techniques. In the following we will discuss some of the most common ones. \n",
    "\n",
    "### More efficient approaches in pattern sliding. Knuth-Morris-Pratt Algorithm (KMP)\n",
    "\n",
    "Both naive approaches don’t work well in cases where we see many matching characters followed by a mismatching character. This is because we still have to try to match a large number of characters before realizing that an exact match is impossible. (Think, e.g. for the case of 'ledzeppelin' in 'Iloveledzeppelix'). \n",
    "\n",
    "The KMP matching algorithm focus **again** on the _pattern_ and in particular specific structural properties of the pattern. For instance if the pattern has recurring sub-patterns appearing more than once. The basic idea behind KMP’s algorithm is: whenever we detect a mismatch (after some matches), we already know some of the characters in the text of the next window. We take advantage of this information to avoid matching the characters that we know will anyway match.  \n",
    "  \n",
    "For instance check the following example:\n",
    "\n",
    "sequence : XXXXAAAAAXXBB\n",
    "pattern : AAA\n",
    "\n",
    "The _pattern_ will match three consecutive positions in sequence but because its structure is repetitive, once we have matched it for the first time, we don't need to match the first two positions in the second match. We can simply move to the third position and check that one to ascertain the match. \n",
    "\n",
    "#### Preprocessing in KMP\n",
    "KMP uses the structure of the pattern to allow for efficient slides like the one above. It does so by preprocessing the pattern. The idea of preprocessing is to create \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found pattern at index 10\n"
     ]
    }
   ],
   "source": [
    "# Python program for KMP Algorithm\n",
    "def KMPSearch(pat, txt):\n",
    "    M = len(pat)\n",
    "    N = len(txt)\n",
    "  \n",
    "    # create lps[] that will hold the longest prefix suffix \n",
    "    # values for pattern\n",
    "    lps = [0]*M\n",
    "    j = 0 # index for pat[]\n",
    "  \n",
    "    # Preprocess the pattern (calculate lps[] array)\n",
    "    computeLPSArray(pat, M, lps)\n",
    "  \n",
    "    i = 0 # index for txt[]\n",
    "    while i < N:\n",
    "        if pat[j] == txt[i]:\n",
    "            i += 1\n",
    "            j += 1\n",
    "  \n",
    "        if j == M:\n",
    "            print(\"Found pattern at index \" + str(i-j))\n",
    "            j = lps[j-1]\n",
    "  \n",
    "        # mismatch after j matches\n",
    "        elif i < N and pat[j] != txt[i]:\n",
    "            # Do not match lps[0..lps[j-1]] characters,\n",
    "            # they will match anyway\n",
    "            if j != 0:\n",
    "                j = lps[j-1]\n",
    "            else:\n",
    "                i += 1\n",
    "  \n",
    "def computeLPSArray(pat, M, lps):\n",
    "    len = 0 # length of the previous longest prefix suffix\n",
    "  \n",
    "    lps[0] # lps[0] is always 0\n",
    "    i = 1\n",
    "  \n",
    "    # the loop calculates lps[i] for i = 1 to M-1\n",
    "    while i < M:\n",
    "        if pat[i]== pat[len]:\n",
    "            len += 1\n",
    "            lps[i] = len\n",
    "            i += 1\n",
    "        else:\n",
    "            # This is tricky. Consider the example.\n",
    "            # AAACAAAA and i = 7. The idea is similar \n",
    "            # to search step.\n",
    "            if len != 0:\n",
    "                len = lps[len-1]\n",
    "  \n",
    "                # Also, note that we do not increment i here\n",
    "            else:\n",
    "                lps[i] = 0\n",
    "                i += 1\n",
    "  \n",
    "txt = \"ABABDABACDABABCABAB\"\n",
    "pat = \"ABABCABAB\"\n",
    "KMPSearch(pat, txt)"
   ]
  },
  {
   "source": [
    "BM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python3 Program for Bad Character Heuristic\n",
    "# of Boyer Moore String Matching Algorithm\n",
    " \n",
    "NO_OF_CHARS = 256\n",
    " \n",
    "def badCharHeuristic(string, size):\n",
    "    '''\n",
    "    The preprocessing function for\n",
    "    Boyer Moore's bad character heuristic\n",
    "    '''\n",
    " \n",
    "    # Initialize all occurrence as -1\n",
    "    badChar = [-1]*NO_OF_CHARS\n",
    " \n",
    "    # Fill the actual value of last occurrence\n",
    "    for i in range(size):\n",
    "        badChar[ord(string[i])] = i;\n",
    " \n",
    "    # retun initialized list\n",
    "    return badChar\n",
    " \n",
    "def search(txt, pat):\n",
    "    '''\n",
    "    A pattern searching function that uses Bad Character\n",
    "    Heuristic of Boyer Moore Algorithm\n",
    "    '''\n",
    "    m = len(pat)\n",
    "    n = len(txt)\n",
    " \n",
    "    # create the bad character list by calling\n",
    "    # the preprocessing function badCharHeuristic()\n",
    "    # for given pattern\n",
    "    badChar = badCharHeuristic(pat, m)\n",
    " \n",
    "    # s is shift of the pattern with respect to text\n",
    "    s = 0\n",
    "    while(s <= n-m):\n",
    "        j = m-1\n",
    " \n",
    "        # Keep reducing index j of pattern while\n",
    "        # characters of pattern and text are matching\n",
    "        # at this shift s\n",
    "        while j>=0 and pat[j] == txt[s+j]:\n",
    "            j -= 1\n",
    " \n",
    "        # If the pattern is present at current shift,\n",
    "        # then index j will become -1 after the above loop\n",
    "        if j<0:\n",
    "            print(\"Pattern occur at shift = {}\".format(s))\n",
    " \n",
    "            '''   \n",
    "                Shift the pattern so that the next character in text\n",
    "                      aligns with the last occurrence of it in pattern.\n",
    "                The condition s+m < n is necessary for the case when\n",
    "                   pattern occurs at the end of text\n",
    "               '''\n",
    "            s += (m-badChar[ord(txt[s+m])] if s+m<n else 1)\n",
    "        else:\n",
    "            '''\n",
    "               Shift the pattern so that the bad character in text\n",
    "               aligns with the last occurrence of it in pattern. The\n",
    "               max function is used to make sure that we get a positive\n",
    "               shift. We may get a negative shift if the last occurrence\n",
    "               of bad character in pattern is on the right side of the\n",
    "               current character.\n",
    "            '''\n",
    "            s += max(1, j-badChar[ord(txt[s+j])])\n",
    " \n",
    " \n",
    "# Driver program to test above function\n",
    "def main():\n",
    "    txt = \"ABAAABCD\"\n",
    "    pat = \"ABC\"\n",
    "    search(txt, pat)\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "source": [
    "FASTA\n",
    "\n",
    "FastA - Steps\n",
    "Hashing: FastA locates regions of the query sequence and matching regions in the database sequences that have high densities of exact word matches. (without gaps) The length of the matched word is called the ktup parameter.\n",
    "Scoring: The ten highest scoring regions are rescored using a scoring matrix. The score for such a pair of regions is saved as the init1 score.\n",
    "Introduction of Gaps: FastA determines if any of the initial regions from different diagonals may be joined together to form an approximate alignment with gaps. Only non-overlapping regions may be joined. The score for the joined regions is the sum of the scores of the initial regions minus a joining penalty for each gap. The score of the highest scoring region, at the end of this step, is saved as the initn score.\n",
    "Alignment: After computing the initial scores, FastA determines the best segment of similarity between the query sequence and the search set sequence, using a variation of the Smith-Waterman algorithm. The score for this alignment is the opt score.\n",
    "Random Sequence Simulation: In order to evaluate the significance of such alignment FastA empirically estimates the score distribution from the alignment of many random pairs of sequences. More precisely, the characters of the query sequences are reshuffled (to maintain bias due to length and character composition) and searched against a random subset of the database. This empirical distribution is extrapolated, assuming it is an extreme value distribution, and each alignment to the real query is assigned a Z-score and an E-score."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a PSSM\n",
    "def pssm(pwm, nucfreqs):\n",
    "    import numpy as np\n",
    "    import math\n",
    "    pseudocount=0.01\n",
    "    pssm=[[0 for i in range(len(pwm[0]))] for j in range(len(nucfreqs))]\n",
    "    for i in range(len(nucfreqs)):\n",
    "        pssm[i]=(np.array(pwm[i])+pseudocount)/nucfreqs[i]\n",
    "    for i in range(len(pssm)):\n",
    "        for k in range(len(pssm[0])):\n",
    "            pssm[i][k]=math.log(pssm[i][k])/math.log(2)\n",
    "    return(pssm)\n",
    "\n",
    "mypssm=pssm(mypwm, nucfreqs)"
   ]
  },
  {
   "source": [
    "c. Searching a sequence with a PWM/PSSM or with Hamming Distance "
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pssmSearch(pssm, sequence, threshold):\n",
    "    nuc = ['A', 'C', 'G', 'T']\n",
    "    hits = []\n",
    "    instances = []\n",
    "    for i in range(len(sequence)-len(pssm[0])):\n",
    "        instance=sequence[i:i+len(pssm[0])]\n",
    "        score=0\n",
    "        for l in range(len(instance)):\n",
    "            score=score+pssm[nuc.index(instance[l])][l]\n",
    "        if (score > threshold):\n",
    "            hits.append(i)\n",
    "            instances.append(instance) \n",
    "    return(hits, instances)\n",
    "\n",
    "out=pssmSearch(mypssm, targetsequence, 9)"
   ]
  },
  {
   "source": [
    "BLAST"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to be added"
   ]
  },
  {
   "source": [
    "### The next problem. Discover a new motif from a given set of sequences\n",
    "\n",
    "#### Part 1. Formulating the problem\n",
    "1. Given a set of sequences that each contains an instance of the motif, find the motif.\n",
    "\n",
    "#### A first approach \n",
    "\n",
    "Assuming we have a way to \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "source": [
    "1. Given a set of s sequences: Find a set of k-mers (for a given\n",
    "length k, one from each sequence) that maximizes the score (or\n",
    "minimizes the distance) of each (one) k-mer with its sequence\n",
    "2. Collect k-mers\n",
    "3. Create a motif from them\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "source": [
    "### Brute Force Approach\n",
    "\n",
    "What is the complexity of the BFA?\n",
    "1. Number of k-mers 4k\n",
    "2. Number of k-mers in each sequence: (n − k + 1)\n",
    "3. Number of calculations for each k-mer given s sequences of\n",
    "length n: (n − k + 1)s\n",
    "4. Total number of calculations 4k (n − k + 1)s\n",
    "\n",
    "The complexity of the algorithm is at least O(ns ).\n",
    "\n",
    "We need something faster!\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "source": [
    "* Assuming we have a way to calculate the distance of a k-mer k\n",
    "\n",
    "```\n",
    "from a given sequence seq  \n",
    "    for k in kmers:  \n",
    "        for seq in sequences:  \n",
    "            if distance(k, seq)<min_distance:  \n",
    "                min_distance<-distance(k,seq)  \n",
    "                motif[seq]<-k\n",
    "```\n",
    "\n",
    "* Because each k-mer needs to pass only once through each\n",
    "sequence, the median string has O(4k ) complexity because k is\n",
    "(usually) much shorter than the length of the sequence.\n",
    "\n",
    "* However, it is still quite slow and for k>10 its implementation\n",
    "is still unapplicable.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "source": [
    "### An alternative\n",
    "\n",
    "* Assume a greedy approach to go through all sequences\n",
    "updating a motif every time\n",
    "\n",
    "* Starting from sequence i:\n",
    "\n",
    "1. find the most common k-mer\n",
    "2. create a profile from it (adding pseudocounts to all 0-values)\n",
    "3. go to the next sequence\n",
    "4. choose the k-mer that best fits the profile\n",
    "5. store that k-mer in the collection and update profile\n",
    "6. iterate steps 3->5.\n",
    "\n",
    "* We’ ve just described a Greedy approach for discovering a\n",
    "motif p of a given length k among t sequences.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "source": [
    "### A greedy Approach\n",
    "\n",
    "* Assuming a set of s sequences and a given consensus k-mer k:\n",
    "\n",
    "* We will construct a PWM “on the go” as we move from one\n",
    "sequence to the next.\n",
    "1. For i=1 :\n",
    "2. For each k in seq_i:  \n",
    "    2.1 For i = 2 to i = s:  \n",
    "    2.2 Find the best (smallest distance) kmer in seq_i  \n",
    "    2.3 Build a profile  \n",
    "    2.4 If the score(profile) is better than all previous update profile  \n",
    "3. Repeat\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Analysis. Greedy Approach\n",
    "\n",
    "1. Why Greedy: It takes kmers from the first sequence only to\n",
    "scan in the following. Thus it doesn’t go through all\n",
    "combinations of sequences and k-mers. As we’ve seen above\n",
    "the trade-off is speed.\n",
    "2. KEY: It assumes that all sequences contain the motif. If the\n",
    "first sequence doesn’t contain the motif (in any variation) then\n",
    "we are doomed in looking for something that is non-sensical.\n",
    "3. A way to go around this is to sample a small percentage of\n",
    "sequences randomly, which brings us to the next-to-last chapter\n",
    "of the motif finding problem\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### A randomized approach\n",
    "\n",
    "* In the Greedy Approach we take the kmers from the first\n",
    "sequence and scan over the rest. In this way an initial wrong\n",
    "choice may lead you to disastrous results.\n",
    "* In a Randomized Approach we start, instead with a\n",
    "collection of s k-mers, one from each sequence, build a profile,\n",
    "scan the sequences with that profile, update it and repeat until\n",
    "the k-mer set is good enough match for the updated profile.\n",
    "* Stop and think of the problems we get rid of with this\n",
    "approach.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Pseudocode\n",
    "\n",
    "```\n",
    "for seq in sequences:  \n",
    "    profile[seq]<-random(k, seq)  \n",
    "    while distance(profile, sequences)>threshold  \n",
    "        for seq in sequences:  \n",
    "        profile[seq]<-max(k, profile, seq)  \n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  }
 ]
}